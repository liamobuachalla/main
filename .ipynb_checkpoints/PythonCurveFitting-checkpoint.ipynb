{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd4d60e",
   "metadata": {},
   "source": [
    "# Curve Fitting\n",
    "Curve fitting is an optimization process of constructing a curve, or mathematical function, that has the best fit to a series of data points that may be subject to constraints. The process of curve fitting can involve interpolation, the process of estimating new data points in the range of a discrete set of known data points. Curve fitting can assist with extrapolation, the process of estimating new data points outside of the range of known data points.\n",
    "\n",
    "Python implements curve fitting through a function located in the `scipy.optimize` package. By using the function `curve_fit`, you can define a model fit function and method of fit, and obtain the best fit parameters and a covariance matrix for the parameters.\n",
    "\n",
    "## Methods of Fit\n",
    "`curve_fit` uses one of 3 methods to fit data. The fitting method is specified in the `curve-fit` function using the argument, `method='lm'`. \n",
    "\n",
    "### `lm`\n",
    "The default method is the Levenberg-Marquardt algorithm, also called the damped least-squares. The LM method interpolates between the Gauss-Newton algorithm and the method of gradient descent. In general terms, given $m$ pairs of data $(x_i,y_i)$, it finds the parameter(s) $\\beta$ of the model function $f(x,\\beta)$ that minimizes the sum of the squares of the deviations, that is \n",
    "$$\\sum_{i=1}^{m}[y_i-f(x_i,\\beta)]^2$$\n",
    "The algorithm does this through an iterative process. Note that this algorithm will no work if the number of points $m$ is less than the number of parameters in the function.\n",
    "\n",
    "### `trf`\n",
    "The second method is known as the *trust region method*. This method fits the parameters using a least-squares-like measurement in defined regions of the data. These regions expand and contract based on the goodness of fit in the current region. These sets of algorithms are also referred to as *restricted-step methods*\n",
    "\n",
    "### `dogbox`\n",
    "The last method is the *Powell's dog leg method*. This method is an iterative optimisation algorithm silmilar to the Levenberg-Marquardt algorithm, but it uses a explicit trust region in each iteration. In each step, it searches for a minimum in the objective functions along the steepest descent direction. Its name comes from the resemblance between the construction of the dog leg step and the shape of a [dogleg hole](https://en.wikipedia.org/wiki/Golf_course#Fairway_and_rough) in golf\n",
    "\n",
    "\n",
    "## How do you go about fitting the statistical model to the data?\n",
    "\n",
    "Let's consider some alpha particle decay data, it is contained in the `alphadecay.txt` file. The file contains 3 columns: time of measurement (in days), rate of decay (in detections per second), and the uncertainty in the rate of decay. The rate of decay is described by the function\n",
    "$$R(t) = R_0 e^{-t/\\tau}$$  \n",
    "where $\\tau$ is the mean lifetime of the alpha particle. In an ideal world, we could isolate the laboratory completely, however, some of the measured alpha decays in the data set are from background radioactivity. Instead, we can describe this data by adding a parameter,\n",
    "$$R(t) = R_0 e^{-t/\\tau} + R_{bkgd}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as sy\n",
    "\n",
    "decayfile = pd.read_table('data/alphadecay.txt', sep= '\\s+', header=None)\n",
    "x_time = decayfile.loc[:,0]\n",
    "y_rate = decayfile.loc[:,1]\n",
    "#define fit function\n",
    "def decayfit(x, a, b, c):\n",
    "    return a*np.exp(-x/b)+c\n",
    "#fit \n",
    "param, covar = sy.curve_fit(decayfit, x_time, y_rate, p0=[2.1892,1,0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean lifetime\n",
    "param[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a68e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covariance matrix\n",
    "covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae26e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mp\n",
    "#plot data and fit function\n",
    "mp.scatter(x_time, y_rate)\n",
    "mp.plot(x_time, decayfit(x_time,param[0],param[1],param[2]),'r')\n",
    "#plot constant background\n",
    "mp.plot(x_time,param[2]*np.ones(len(x_time)),'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fda754",
   "metadata": {},
   "source": [
    "## Covariant Matrix\n",
    "The second parameter returned by `curve_fit` is the [covariance matrix](https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices) for the fit parameters. The covariance matrix is a measure of the uncertainty in the parameter estimates. It is a square matrix with dimensions equal to the number of parameters in the fit function. The diagonal elements of the matrix are the variances of the individual parameters, or the uncertainty in those estimates. The off-diagonal elements are the covariances between the parameters, which represent the degree of correlation between the estimates of those parameters. The elements themselves hold very little information, but statistics derived from them are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da400eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce8a51",
   "metadata": {},
   "source": [
    "Many sources use a colormap to get an impression of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b85d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d435ce8",
   "metadata": {},
   "source": [
    "The correlation between two parameters can be calculated. This tells you how independent they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c651b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Rsquared = covar[0,2]/np.sqrt(covar[0,0]*covar[2,2])\n",
    "print(Rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7589a05",
   "metadata": {},
   "source": [
    "# Particle Physics Example\n",
    "Let's look through the file `dimuon.csv`. This file contains data about particle collisions resulting in two muons. The columns contain the energy, momentum, and charge of each muon pair. The last column `M` contains the invariant mass calculation of the parent particle in the interaction: $X\\rightarrow \\mu^+\\mu^-$. By analyzing the invariant mass values, you can find the identity of particle $X$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/dimuon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the data\n",
    "data.hist('M',bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfe394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a cut on the data\n",
    "data[(data.M<3.6) & (data.M>2.5)].hist('M',bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus = data[(data.M<3.6) & (data.M>2.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccfb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the fit to Breit-Wigner function\n",
    "def bw(x,k,M,g):\n",
    "    return k/((x**2-M**2)**2 + g**2*M**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4220f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parameters of the histogram to put in curve fit\n",
    "counts, bins = np.histogram(focus.M,bins=1000)\n",
    "newbins = np.zeros(len(counts))\n",
    "for k in range(0,len(bins)-1):\n",
    "    newbins[k] = (bins[k]+bins[k+1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "muparams,mucovar = sy.curve_fit(bw,newbins,counts,p0 = [300,3.9,0.25],method='lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ca8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "muparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52594b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mucovar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how the fit appears in graphs\n",
    "mp.plot(newbins,bw(newbins,muparams[0],muparams[1],muparams[2]))\n",
    "mp.hist(focus.M,bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.pt2<20].hist('pt2',bins=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277c972",
   "metadata": {},
   "source": [
    "# Is this curve fitting or linear regression?\n",
    "The distinction between curve fitting and linear regression is fuzzy. In my opinion, curve fitting becomes linear regression when you have a hypothesis about how the data is generated (what physical phenomena is producing the data) and how the parameters relate to one another, and when you dig deeper into the statistical analysis of the parameters and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df615d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
